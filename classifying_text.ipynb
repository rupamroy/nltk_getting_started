{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# This function will go to doxydonkey.blogspot.in and recursively find all the links to the all the posts\n",
    "# unless no older posts is found\n",
    "def getAllDoxyDonkeyPosts(url, links):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(response)\n",
    "    for a in soup.findAll('a'):\n",
    "        try:\n",
    "            url = a['href']\n",
    "            title = a['title']\n",
    "            if title == 'Older Posts':\n",
    "                print(title, url)\n",
    "                links.append(url)\n",
    "                getAllDoxyDonkeyPosts(url, links)\n",
    "        except:\n",
    "            title = \"\"\n",
    "    return \n",
    "\n",
    "\n",
    "blogUrl = \"http://doxydonkey.blogspot.in\"\n",
    "links = []\n",
    "getAllDoxyDonkeyPosts(blogUrl, links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to parse the article texts from each link\n",
    "# each article is under an <li> tag and each post may have multiple <li> , and they are in div.class=\"postBody\"\n",
    "def getDoxyDonkeyText(testUrl):\n",
    "    response = urllib.request.urlopen(testUrl)\n",
    "    soup = BeautifulSoup(response)\n",
    "    mydivs = soup.findAll('div', {'class': 'post-body'})\n",
    "    \n",
    "    posts = []\n",
    "    for div in mydivs:\n",
    "        posts += map(lambda p: p.text.replace('\\xa0',' '), div.findAll('li'))\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doxyDonkeyPosts =[]\n",
    "\n",
    "for link in links:\n",
    "    doxyDonkeyPosts +=  getDoxyDonkeyText(link)\n",
    "    \n",
    "doxyDonkeyPosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts text to TF-IDF representation \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2804x13219 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 280830 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taks a list of documents and returns a two-dim array in which each row represent one document\n",
    "X = vectorizer.fit_transform(doxyDonkeyPosts)\n",
    "# X # outputs a matrix of 2804*13219 - where 2804 is the number of articles in our corpus\n",
    "# 13219 - the number of columns represents the number of distinct words which are presnet in all the articles. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# init - alghorithm to choose the centroids in such a way so that it takes min iterations to get the a convergence.\n",
    "# For are other algorithms choices which you can look up in the documentation\n",
    "# n_init = \n",
    "km = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 100, n_init = 1, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 5234.044\n",
      "Iteration  1, inertia 2673.358\n",
      "Iteration  2, inertia 2667.329\n",
      "Iteration  3, inertia 2666.629\n",
      "Iteration  4, inertia 2666.489\n",
      "Iteration  5, inertia 2666.454\n",
      "Iteration  6, inertia 2666.441\n",
      "Iteration  7, inertia 2666.438\n",
      "Iteration  8, inertia 2666.434\n",
      "Iteration  9, inertia 2666.430\n",
      "Iteration 10, inertia 2666.427\n",
      "Converged at iteration 10: center shift 0.000000e+00 within tolerance 7.308517e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=3, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# runs the algorithm for our data\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2], dtype=int32), array([1842,  404,  558]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every document in our list has been assigned a number for the cluster to which it belongs \n",
    "# all of that will be stored in the poperty labels_ ofthe km object\n",
    "import numpy as np\n",
    "np.unique(km.labels_, return_counts=True)\n",
    "# Outputs (array([0, 1, 2], dtype=int32), array([1842,  404,  558]))\n",
    "# Here 0, 1 and 2 are the labbels for each of the cluster since we had mentioned three clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets articulate what those themes could be based on which the clustering has been done\n",
    "text={}\n",
    "# this will be a dictionary where keys will be cluster numbers and the values will be the\n",
    "# aggregated text across all the articles present in that cluster\n",
    "for i, cluster in enumerate(km.labels_):\n",
    "    oneDocument= docyDonkeysPosts[i]\n",
    "    if cluster not in text.keys():\n",
    "        text[cluster] = oneDocument\n",
    "    else:\n",
    "        text[cluster] += oneDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we find the most frequent words in each cluster\n",
    "from nltk.tokenize import sent_tokeniz, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stopwords = set(stopwords.words('english') + list(punctuation) + ['“', '”', '\\'s', '', '’', 'y/y', 'millions', 'billions', 'year', 'million', 'billion'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top keywords in each cluster and their counts\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
