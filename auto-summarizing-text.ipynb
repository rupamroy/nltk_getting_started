{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "articleURL=\"https://www.washingtonpost.com/news/the-switch/wp/2016/10/18/the-pentagons-massive-new-telescope-is-designed-to-track-space-junk-and-watch-out-for-killer-asteroids/?utm_term=.da4e0c017d62\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with urllib.request.urlopen(articleURL) as url:\n",
    "    page = url.read().decode('utf8', 'ignore') # downloads the page\n",
    "soup = BeautifulSoup(page, \"lxml\") # takes the html and represents as a tag tree\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to extract text we need to know the sctructure of the tags.\n",
    "# By convention we know that washington post puts all its text in article tags\n",
    "ar = soup.find('article') # Finds the first article\n",
    "# ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "arText = soup.find('article').text # get the text within the article\n",
    "# arText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find and join all the articles in the web page we use the soup.find_all method\n",
    "text = ' '.join(map(lambda p: p.text, soup.find_all('article')))\n",
    "#text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below lines removes encoded special characters like \\xa0\n",
    "cleanText=text.replace('\\xa0',' ')\n",
    "#cleanText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are compiling all these logic to download and parse text into one singke function\n",
    "def getTextWaPo(url): # function get text from washington post\n",
    "    with urllib.request.urlopen(articleURL) as url:\n",
    "        page = url.read().decode('utf8', 'ignore') # downloads the page\n",
    "    soup = BeautifulSoup(page, \"lxml\") # takes the html and represents as a tag tree\n",
    "    text = ' '.join(map(lambda p: p.text, soup.find_all('article')))\n",
    "    return text.replace('\\xa0',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=getTextWaPo(articleURL) # down and clear text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we process the text , we breakdown the text into sentences and words\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = sent_tokenize(text) # break article into sentences. \n",
    "# Remember that any period should be followed by a space to be considerd a seperate sentence\n",
    "# sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sent=word_tokenize(text.lower()) # we get the list of words in the entire article\n",
    "customStopwords = set(stopwords.words('english') + list(punctuation) + ['“', '”', '\\'s', '', '’']) # stopwords and punctuations\n",
    "word_sent = [word for word in word_sent if word not in customStopwords]\n",
    "# word_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'space': 15, 'telescope': 9, 'objects': 7, 'debris': 7, 'satellites': 6, 'orbit': 6, 'air': 6, 'force': 6, 'around': 4, 'small': 4, ...})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we are first going  to find the most important words\n",
    "from nltk.probability import FreqDist # FreqDist is a table with words in one column and number of times \n",
    "# the words occur in another table\n",
    "freq = FreqDist(word_sent)\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['space',\n",
       " 'telescope',\n",
       " 'objects',\n",
       " 'debris',\n",
       " 'satellites',\n",
       " 'orbit',\n",
       " 'air',\n",
       " 'force',\n",
       " 'around',\n",
       " 'small']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will find the most important words\n",
    "from heapq import nlargest # can be used to sort lists and collections abnd retrun the top n values\n",
    "nlargest(10, freq, key=freq.get) # no of elements you want to pick, the collection,\n",
    "# a function which can be used to sort the collection, the function that we spefified get is freq.get which returns \n",
    "# the value in the dictionary for the given key\n",
    "# this is just a demo of the function - we will use it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {0: 27,\n",
       "             1: 23,\n",
       "             2: 7,\n",
       "             3: 34,\n",
       "             4: 17,\n",
       "             5: 41,\n",
       "             6: 65,\n",
       "             7: 3,\n",
       "             8: 50,\n",
       "             9: 24,\n",
       "             10: 10,\n",
       "             11: 12,\n",
       "             12: 15,\n",
       "             13: 100,\n",
       "             14: 45,\n",
       "             15: 45,\n",
       "             16: 67,\n",
       "             17: 28,\n",
       "             18: 87,\n",
       "             19: 35,\n",
       "             20: 9,\n",
       "             21: 21,\n",
       "             22: 38,\n",
       "             23: 26})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now that we have the most importnat words, we can use that to assign a significance score to each sentence\n",
    "# in the article\n",
    "from collections import defaultdict # we will create a dic with key = sentences , value = significance score\n",
    "# default dict will not throw error if key not find , but will add that key to the dictionary\n",
    "ranking = defaultdict(int)\n",
    "\n",
    "for i, sent in enumerate(sents): # enumerate converts [a,b,c] to [(0,a),(1,b),(2,c)] which goes into values i, sent\n",
    "    for w in word_tokenize(sent):\n",
    "        if w in freq:\n",
    "            ranking[i] += freq[w]\n",
    "ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 18, 16, 6]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we now use the nlargest to pick the top 4 sentences with the largest significance score\n",
    "sents_idx = nlargest(4, ranking, key=ranking.get)\n",
    "sents_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On Tuesday, the Defense Department took another significant step toward monitoring all of the cosmic junk swirling around in space, by delivering a gigantic new telescope capable of seeing small objects from very far away.',\n",
       " \"But the telescope's ability to see “something very far away over a very wide area is really what it’s best at.” DARPA says the advanced technology in the massive, 90-ton telescope would allow officials to go from “seeing only a few large objects at a time through the equivalent of a drinking straw to a windshield view with 10,000 objects at a time.” It is also being used by NASA to monitor asteroids and other near-Earth objects that could collide with the planet, officials said.\",\n",
       " \"“That's a critical capability for the U.S. military, as they have a lot of very important satellites in GEO, and are increasingly worried about threats to those satellites.” The telescope would join another new space debris tracking technology known as the Space Fence, which is now being built by Bethesda-based Lockheed Martin.\",\n",
       " 'With many valuable assets in space—satellites used for intelligence, communications and guiding weapons—the Pentagon has become increasingly concerned with what it calls “space situational awareness.” Instead of being a benign environment, the Pentagon likes to say that space has become “contested, congested and competitive.” In orbit, debris moves very fast, as much as 17,500 m.p.h., so that even a fleck of paint could cause damage.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we now pick the sentences which are presnet in this list but sorted in asc order. \n",
    "[sents[j] for j in sorted(sents_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we suimmarize what we have done so far in a function\n",
    "def summarize(text, n):\n",
    "    sents = sent_tokenize(text)\n",
    "    \n",
    "    assert n <= len(sents)\n",
    "    word_sent=word_tokenize(text.lower()) # we get the list of words in the entire article\n",
    "    customStopwords = set(stopwords.words('english') + list(punctuation) + ['“', '”', '\\'s', '', '’']) # stopwords and punctuations\n",
    "    word_sent = [word for word in word_sent if word not in customStopwords]\n",
    "    # word_sent\n",
    "    freq = FreqDist(word_sent)\n",
    "    \n",
    "    ranking = defaultdict(int)\n",
    "\n",
    "    for i, sent in enumerate(sents): # enumerate converts [a,b,c] to [(0,a),(1,b),(2,c)] which goes into values i, sent\n",
    "        for w in word_tokenize(sent):\n",
    "            if w in freq:\n",
    "                ranking[i] += freq[w]\n",
    "    \n",
    "    sents_idx = nlargest(4, ranking, key=ranking.get)\n",
    "    return [sents[j] for j in sorted(sents_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On Tuesday, the Defense Department took another significant step toward monitoring all of the cosmic junk swirling around in space, by delivering a gigantic new telescope capable of seeing small objects from very far away.',\n",
       " \"But the telescope's ability to see “something very far away over a very wide area is really what it’s best at.” DARPA says the advanced technology in the massive, 90-ton telescope would allow officials to go from “seeing only a few large objects at a time through the equivalent of a drinking straw to a windshield view with 10,000 objects at a time.” It is also being used by NASA to monitor asteroids and other near-Earth objects that could collide with the planet, officials said.\",\n",
       " \"“That's a critical capability for the U.S. military, as they have a lot of very important satellites in GEO, and are increasingly worried about threats to those satellites.” The telescope would join another new space debris tracking technology known as the Space Fence, which is now being built by Bethesda-based Lockheed Martin.\",\n",
       " 'With many valuable assets in space—satellites used for intelligence, communications and guiding weapons—the Pentagon has become increasingly concerned with what it calls “space situational awareness.” Instead of being a benign environment, the Pentagon likes to say that space has become “contested, congested and competitive.” In orbit, debris moves very fast, as much as 17,500 m.p.h., so that even a fleck of paint could cause damage.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(text,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
